{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehrerm/TFM/blob/main/notebooks/01_load_and_clean_DIRAC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWP9zGef0S1a"
      },
      "source": [
        "# Creación de modelos de puntuación del Riesgo de Impago\n",
        "\n",
        "En esta aplicación implementaremos un modelo de puntuación de riesgo poblacional a la mortalidad debido a los cánceres más comunes y su relación con las cantidades y tecnologías usadas en radioterapia.\n",
        "\n",
        "\n",
        "riesgo de impago siguiendo la metodología  que hemos analizado en la presentación de clase (selección de variables mediante valor de la información, tramificación de variables continuas, agrupación de categorías, transformación woe de variables, estimación de modelos de regresión logística, ....)\n",
        "\n",
        "Existen diferentes librerías que incorporan funciones con los diferentes procedimientos ya programados que nos facilitan mucho la tarea. Una de estas librerías es `scorecardpy` [librería scorecardpy](https://pypi.org/project/scorecardpy/) que estima tarjetas de puntuación *lineales* utilizando regresiones logísticas. Esta librería nació inicialmente en R, y lamentablemente la versión de Python da algunos errores de adaptación a las últimas versiones de Pandas. Su desarrollador remite a utilizar la versión estable de R (librería en R 'scorecard').\n",
        "\n",
        "Así que en su lugar de esta librería utilizaremos la librería `optBinning` [librería OptBinning](http://gnpalencia.org/optbinning/) que en realidad recoge (y en mi opinión mejora) la principal función de la librería `scorecardpy`  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV_QVJo-0S1c"
      },
      "source": [
        "## Creación de un entorno e instalación de librerías\n",
        "# como en este caso usaremos Colab, no necesitaremos instalar librerías, pero sí cargarlas en este entorno. Se procuarará importar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5TgTn0u0S1e"
      },
      "outputs": [],
      "source": [
        "#Cargo o importo pandas, numpy, Matplotlib,\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Para el análisis descriptivo inicial de contraste de asociación importo el test Chi2 y el anova\n",
        "from scipy.stats import chi2_contingency\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "\n",
        "# Librería para hacer la tramificación, agrupación y transformación WOE\n",
        "from optbinning import Scorecard, BinningProcess, OptimalBinning\n",
        "from optbinning.scorecard import plot_auc_roc, plot_cap, plot_ks, ScorecardMonitoring\n",
        "\n",
        "# Scikit-learn para dividir la muestra y para estimar el modelo de regresión logística (sólo si no se quiere utilizar\n",
        "# la función optbinning.scorecard que ya lo incropora)\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import balanced_accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iva1X4gu0S1f"
      },
      "source": [
        "# Carga, Exploración y Preparación de los datos sobre equipos de radioterapia y pacientes que utilizaremos\n",
        "## Carga de los datos\n",
        "Se utilizará una base de datos de créditos de la IAEA junto con los de la OMS, son datos reales, se buscará primeramente un estudio de un solo año (2022) ya que se busca algo lo más actual, completo y que tenga lo menos posible la influencia del COVID-19.\n",
        "\n",
        " Eso significa que las magnitudes de cantidades (expresadas en Marcos Alemanes), sean de difícil interpretación para el día de hoy. Sin embargo el signo y el sentido de las variables utilizadas para predecir el riesgo de impago de los futuros clientes permanece todavía de plena utilidad.\n",
        "\n",
        "Los datos pueden descargarse en la página de la IAEA en el apartado de DIRAC https://dirac.iaea.org/Query/Countries  donde aparecen reflejados los datos de los equipos de radioterapia y el año de sus ultimas actualizaciones q nivel the hardware, por otro lado, se descargaron los datos de la OMS, en el Global Cancer Observatory, donde se descargaron tanto las incidencias como las mortalidades por cáncer a nivel mundial https://gco.iarc.fr/overtime/en/dataviz/trends?populations=752&sexes=1_2&types=1&multiple_populations=1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppNnYmRo0S1f"
      },
      "outputs": [],
      "source": [
        "# Cargamos los datos\n",
        "dt=pd.read_excel('DatosPractica_Scoring.xlsx', engine='openpyxl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiKGunQv0S1f"
      },
      "source": [
        "## Descripción inicial de los datos\n",
        "Vamos a hacer una descripción inicial de los datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR6pXRDA0S1f",
        "outputId": "6a3157b7-3a7f-4c05-8eb6-ce07d79e2de8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1319 entries, 0 to 1318\n",
            "Data columns (total 14 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   ID        1319 non-null   int64  \n",
            " 1   Cardhldr  1285 non-null   float64\n",
            " 2   default   994 non-null    float64\n",
            " 3   Age       1319 non-null   float64\n",
            " 4   Income    1319 non-null   float64\n",
            " 5   Exp_Inc   1319 non-null   float64\n",
            " 6   Avgexp    1319 non-null   float64\n",
            " 7   Ownrent   1319 non-null   int64  \n",
            " 8   Selfempl  1319 non-null   int64  \n",
            " 9   Depndt    1319 non-null   int64  \n",
            " 10  Inc_per   1319 non-null   float64\n",
            " 11  Cur_add   1319 non-null   int64  \n",
            " 12  Major     1319 non-null   int64  \n",
            " 13  Active    1319 non-null   int64  \n",
            "dtypes: float64(7), int64(7)\n",
            "memory usage: 144.4 KB\n"
          ]
        }
      ],
      "source": [
        "#Información del Contenido\n",
        "dt.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZEVGwSJ0S1g"
      },
      "source": [
        "## Variable objetivo: creditability\n",
        "\n",
        " La variable **creditability** es la *calidad crediticia* de  cada cliente, es la variable a predecir. Toma originalmente dos valores (Buen Cliente y Mal Clioente). Esta es la variable objetivo, la variable evento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZli7t6B0S1g",
        "outputId": "888f9bef-12a6-4c0f-8b75-e5ad5d0ce5d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Cardhldr\n",
              "1.0    994\n",
              "0.0    291\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dt[\"Cardhldr\"].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXkaYOY40S1g",
        "outputId": "b42b14f4-580f-40b8-91fe-603c28e768de"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Cardhldr</th>\n",
              "      <th>default</th>\n",
              "      <th>Age</th>\n",
              "      <th>Income</th>\n",
              "      <th>Exp_Inc</th>\n",
              "      <th>Avgexp</th>\n",
              "      <th>Ownrent</th>\n",
              "      <th>Selfempl</th>\n",
              "      <th>Depndt</th>\n",
              "      <th>Inc_per</th>\n",
              "      <th>Cur_add</th>\n",
              "      <th>Major</th>\n",
              "      <th>Active</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.08333</td>\n",
              "      <td>2.4000</td>\n",
              "      <td>0.016798</td>\n",
              "      <td>33.01333</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.400000</td>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>24.25000</td>\n",
              "      <td>3.5000</td>\n",
              "      <td>0.069963</td>\n",
              "      <td>203.89170</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.33333</td>\n",
              "      <td>3.0670</td>\n",
              "      <td>0.159700</td>\n",
              "      <td>408.08250</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.022333</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28.16667</td>\n",
              "      <td>3.3500</td>\n",
              "      <td>0.071625</td>\n",
              "      <td>199.36920</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.350000</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>23.25000</td>\n",
              "      <td>1.8769</td>\n",
              "      <td>0.353630</td>\n",
              "      <td>553.10670</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.876900</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1278</th>\n",
              "      <td>1279</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>41.66667</td>\n",
              "      <td>2.4752</td>\n",
              "      <td>0.067250</td>\n",
              "      <td>138.54670</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.825067</td>\n",
              "      <td>222</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1280</th>\n",
              "      <td>1281</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.66667</td>\n",
              "      <td>5.0500</td>\n",
              "      <td>0.028428</td>\n",
              "      <td>119.30170</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5.050000</td>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1282</th>\n",
              "      <td>1283</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>44.66667</td>\n",
              "      <td>2.4000</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.400000</td>\n",
              "      <td>36</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1283</th>\n",
              "      <td>1284</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>47.41667</td>\n",
              "      <td>4.9200</td>\n",
              "      <td>0.044894</td>\n",
              "      <td>183.89830</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2.460000</td>\n",
              "      <td>288</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1284</th>\n",
              "      <td>1285</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>37.08333</td>\n",
              "      <td>5.4900</td>\n",
              "      <td>0.160214</td>\n",
              "      <td>732.97920</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.745000</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>994 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        ID  Cardhldr  default       Age  Income   Exp_Inc     Avgexp  Ownrent  \\\n",
              "0        1       1.0      0.0  27.08333  2.4000  0.016798   33.01333        0   \n",
              "1        2       1.0      1.0  24.25000  3.5000  0.069963  203.89170        0   \n",
              "3        4       1.0      0.0  40.33333  3.0670  0.159700  408.08250        0   \n",
              "4        5       1.0      0.0  28.16667  3.3500  0.071625  199.36920        1   \n",
              "6        7       1.0      1.0  23.25000  1.8769  0.353630  553.10670        0   \n",
              "...    ...       ...      ...       ...     ...       ...        ...      ...   \n",
              "1278  1279       1.0      0.0  41.66667  2.4752  0.067250  138.54670        0   \n",
              "1280  1281       1.0      0.0  40.66667  5.0500  0.028428  119.30170        1   \n",
              "1282  1283       1.0      0.0  44.66667  2.4000  0.000500    0.00000        0   \n",
              "1283  1284       1.0      0.0  47.41667  4.9200  0.044894  183.89830        1   \n",
              "1284  1285       1.0      1.0  37.08333  5.4900  0.160214  732.97920        1   \n",
              "\n",
              "      Selfempl  Depndt   Inc_per  Cur_add  Major  Active  \n",
              "0            0       0  2.400000       56      1       1  \n",
              "1            0       0  3.500000       60      1      11  \n",
              "3            0       2  1.022333       18      0       0  \n",
              "4            0       0  3.350000       18      1       2  \n",
              "6            0       0  1.876900       12      1       3  \n",
              "...        ...     ...       ...      ...    ...     ...  \n",
              "1278         0       2  0.825067      222      0      10  \n",
              "1280         0       0  5.050000      128      1      10  \n",
              "1282         0       0  2.400000       36      1      11  \n",
              "1283         1       1  2.460000      288      1      14  \n",
              "1284         0       1  2.745000       60      1      19  \n",
              "\n",
              "[994 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Recodifico esta variable creditability para que sea binaria y la llamo \"y\"\n",
        "\n",
        "#dt.rename(columns={\"creditability\":\"y\"},inplace=True)\n",
        "#dt['y'] = dt['y'].replace(['good', 'bad'], [0,1])\n",
        "#Se construirá una tabla donde solamente hayan sido aceptado los clientes.\n",
        "dt_aceptados = dt[dt['Cardhldr'] == 1]\n",
        "display(dt_aceptados)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_AfUICO0S1h",
        "outputId": "53d645cf-c816-4016-8ee3-c62a4a7b0ef4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cortes óptimos: [ 2.5  5.5  6.5  9.5 11.5 18.5]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Bin</th>\n",
              "      <th>Count</th>\n",
              "      <th>Count (%)</th>\n",
              "      <th>Non-event</th>\n",
              "      <th>Event</th>\n",
              "      <th>Event rate</th>\n",
              "      <th>WoE</th>\n",
              "      <th>IV</th>\n",
              "      <th>JS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(-inf, 2.50)</td>\n",
              "      <td>259</td>\n",
              "      <td>0.260563</td>\n",
              "      <td>256</td>\n",
              "      <td>3</td>\n",
              "      <td>0.011583</td>\n",
              "      <td>2.299735</td>\n",
              "      <td>0.595158</td>\n",
              "      <td>0.061393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[2.50, 5.50)</td>\n",
              "      <td>203</td>\n",
              "      <td>0.204225</td>\n",
              "      <td>184</td>\n",
              "      <td>19</td>\n",
              "      <td>0.093596</td>\n",
              "      <td>0.123666</td>\n",
              "      <td>0.002974</td>\n",
              "      <td>0.000372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[5.50, 6.50)</td>\n",
              "      <td>58</td>\n",
              "      <td>0.058350</td>\n",
              "      <td>52</td>\n",
              "      <td>6</td>\n",
              "      <td>0.103448</td>\n",
              "      <td>0.012654</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[6.50, 9.50)</td>\n",
              "      <td>168</td>\n",
              "      <td>0.169014</td>\n",
              "      <td>149</td>\n",
              "      <td>19</td>\n",
              "      <td>0.113095</td>\n",
              "      <td>-0.087323</td>\n",
              "      <td>0.001334</td>\n",
              "      <td>0.000167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[9.50, 11.50)</td>\n",
              "      <td>83</td>\n",
              "      <td>0.083501</td>\n",
              "      <td>72</td>\n",
              "      <td>11</td>\n",
              "      <td>0.132530</td>\n",
              "      <td>-0.26806</td>\n",
              "      <td>0.006667</td>\n",
              "      <td>0.000831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[11.50, 18.50)</td>\n",
              "      <td>168</td>\n",
              "      <td>0.169014</td>\n",
              "      <td>135</td>\n",
              "      <td>33</td>\n",
              "      <td>0.196429</td>\n",
              "      <td>-0.738063</td>\n",
              "      <td>0.122240</td>\n",
              "      <td>0.014942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[18.50, inf)</td>\n",
              "      <td>55</td>\n",
              "      <td>0.055332</td>\n",
              "      <td>42</td>\n",
              "      <td>13</td>\n",
              "      <td>0.236364</td>\n",
              "      <td>-0.97411</td>\n",
              "      <td>0.075795</td>\n",
              "      <td>0.009117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Special</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Missing</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Totals</th>\n",
              "      <td></td>\n",
              "      <td>994</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>890</td>\n",
              "      <td>104</td>\n",
              "      <td>0.104628</td>\n",
              "      <td></td>\n",
              "      <td>0.804177</td>\n",
              "      <td>0.086823</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Bin  Count  Count (%)  Non-event  Event  Event rate  \\\n",
              "0         (-inf, 2.50)    259   0.260563        256      3    0.011583   \n",
              "1         [2.50, 5.50)    203   0.204225        184     19    0.093596   \n",
              "2         [5.50, 6.50)     58   0.058350         52      6    0.103448   \n",
              "3         [6.50, 9.50)    168   0.169014        149     19    0.113095   \n",
              "4        [9.50, 11.50)     83   0.083501         72     11    0.132530   \n",
              "5       [11.50, 18.50)    168   0.169014        135     33    0.196429   \n",
              "6         [18.50, inf)     55   0.055332         42     13    0.236364   \n",
              "7              Special      0   0.000000          0      0    0.000000   \n",
              "8              Missing      0   0.000000          0      0    0.000000   \n",
              "Totals                    994   1.000000        890    104    0.104628   \n",
              "\n",
              "             WoE        IV        JS  \n",
              "0       2.299735  0.595158  0.061393  \n",
              "1       0.123666  0.002974  0.000372  \n",
              "2       0.012654  0.000009  0.000001  \n",
              "3      -0.087323  0.001334  0.000167  \n",
              "4       -0.26806  0.006667  0.000831  \n",
              "5      -0.738063  0.122240  0.014942  \n",
              "6       -0.97411  0.075795  0.009117  \n",
              "7            0.0  0.000000  0.000000  \n",
              "8            0.0  0.000000  0.000000  \n",
              "Totals            0.804177  0.086823  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#dt[\"y\"].value_counts()\n",
        "#ya filtrados los clientes, se calculan las VI\n",
        "# Realizamos la trimificación optima de age.in.years\n",
        "\n",
        "# Definir variable objetivo e independiente\n",
        "\n",
        "# Elige variable y prepara datos\n",
        "#variable = \"Age\"\n",
        "#variable = \"Income\"\n",
        "#variable = \"Exp_Inc\"\n",
        "#variable = \"Avgexp\"\n",
        "#variable = \"Ownrent\"\n",
        "#variable = \"Selfempl\"\n",
        "#variable = \"Depndt\"\n",
        "#variable = \"Inc_per\"\n",
        "#variable = \"Cur_add\"\n",
        "#variable = \"Major\"\n",
        "variable = \"Active\"\n",
        "\n",
        "\n",
        "#dt_modelo = dt_aceptados.dropna(subset=[\"default\", variable])\n",
        "\n",
        "X = dt_aceptados[variable].values\n",
        "Y = dt_aceptados[\"default\"].values\n",
        "\n",
        "# Crear y ajustar el binning\n",
        "optb = OptimalBinning(name=variable, dtype=\"numerical\")\n",
        "optb.fit(X, Y)\n",
        "\n",
        "# Mostrar los splits\n",
        "print(\"Cortes óptimos:\", optb.splits)\n",
        "\n",
        "# Tabla de binning\n",
        "binning_table = optb.binning_table\n",
        "binning_table.build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Rc0f6z50S1h",
        "outputId": "1044e34f-d5b0-4a11-cc8d-d744d0007307"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "y\n",
              "0.0    890\n",
              "1.0    104\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "y\n",
              "0.0    0.895372\n",
              "1.0    0.104628\n",
              "Name: proportion, dtype: float64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dt.rename(columns={\"default\":\"y\"},inplace=True)\n",
        "display(dt[\"y\"].value_counts())\n",
        "dt['y'].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gblljnkw0S1h"
      },
      "source": [
        "**Fíjate que tenemos un 30% de malos clientes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrF3W5zZ0S1h",
        "outputId": "8ae9cca2-08e2-41aa-c238-5d1afc27be4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8953722334004024 0.10462776659959759\n"
          ]
        }
      ],
      "source": [
        "# Guardo los valores para su uso posterior\n",
        "yT_0 = dt['y'].value_counts(normalize=True)[0]\n",
        "yT_1 = dt['y'].value_counts(normalize=True)[1]\n",
        "print(yT_0, yT_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNV2Z8TH0S1h",
        "outputId": "374e69b0-7784-4b32-a7ac-f5b1ab3aba82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 994 entries, 0 to 1284\n",
            "Data columns (total 14 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   ID        994 non-null    int64  \n",
            " 1   Cardhldr  994 non-null    float64\n",
            " 2   y         994 non-null    float64\n",
            " 3   Age       994 non-null    float64\n",
            " 4   Income    994 non-null    float64\n",
            " 5   Exp_Inc   994 non-null    float64\n",
            " 6   Avgexp    994 non-null    float64\n",
            " 7   Ownrent   994 non-null    int64  \n",
            " 8   Selfempl  994 non-null    int64  \n",
            " 9   Depndt    994 non-null    int64  \n",
            " 10  Inc_per   994 non-null    float64\n",
            " 11  Cur_add   994 non-null    int64  \n",
            " 12  Major     994 non-null    int64  \n",
            " 13  Active    994 non-null    int64  \n",
            "dtypes: float64(7), int64(7)\n",
            "memory usage: 116.5 KB\n"
          ]
        }
      ],
      "source": [
        "# También elimino todos los nan\n",
        "dt.dropna(inplace=True)\n",
        "\n",
        "dt.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVF2OVYk0S1i"
      },
      "source": [
        "## Variables Predictoras o explicativas\n",
        "\n",
        "Las 20 restantes variables del data frame (7 numéricas y 13 categóricas) son los atributos o características observadas de esos clientes que se utilizarán para predecir la probabilidad de que los clientes cometan un impago de sis créditos, esto es, de que sean malos clientes. La descripción de estas 20 variables es la siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEGvhCfZ0S1i"
      },
      "outputs": [],
      "source": [
        "#  - Attribute 1: (qualitative) **Status of existing checking account o cuenta corriente**\n",
        "#   - A11 : ... < 0 DM\n",
        "#   - A12 : 0 <= ... < 200 DM\n",
        "#   - A13 : ... >= 200 DM / salary assignments for at least 1 year\n",
        "#   - A14 : no checking account\n",
        "\n",
        "#- Attribute 2: (numerical) **Duration in month**\n",
        "\n",
        "#  - Attribute 3: (qualitative) **Credit history**\n",
        "#     - A30 : no credits taken/ all credits paid back duly (devultos sin mora)\n",
        "#     - A31 : all credits at this bank paid back duly\n",
        "#     - A32 : existing credits paid back duly till now\n",
        "#     - A33 : delay in paying off in the past\n",
        "#     - A34 : critical account/ other credits existing (not at this bank)\n",
        "\n",
        "# - Attribute 4: (qualitative) **Purpose**\n",
        "#     - A40 : car (new)\n",
        "#     - A41 : car (used)\n",
        "#     - A42 : furniture/equipment\n",
        "#     - A43 : radio/television\n",
        "#     - A44 : domestic appliances\n",
        "#     - A45 : repairs\n",
        "#     - A46 : education\n",
        "#     - A47 : (vacation - does not exist?)\n",
        "#     - A48 : retraining\n",
        "#     - A49 : business\n",
        "#    - A410 : others\n",
        "\n",
        "# - Attribute 5: (numerical) **Credit amount**\n",
        "\n",
        "# - Attribute 6: (qualitative) **Savings account/bonds**\n",
        "#    - A61 : ... < 100 DM\n",
        "#    - A62 : 100 <= ... < 500 DM\n",
        "#    - A63 : 500 <= ... < 1000 DM\n",
        "#    - A64 : .. >= 1000 DM\n",
        "#    - A65 : unknown/ no savings account\n",
        "\n",
        "# - Attribute 7: (qualitative) **Present employment since**\n",
        "#     - A71 : unemployed\n",
        "#     - A72 : ... < 1 year\n",
        "#     - A73 : 1 <= ... < 4 years\n",
        "#     - A74 : 4 <= ... < 7 years\n",
        "#     - A75 : .. >= 7 years\n",
        "\n",
        "# - Attribute 8: (numerical) **Installment rate in percentage of disposable income**\n",
        "\n",
        "#  - Attribute 9: (qualitative) **Personal status and sex**\n",
        "#     - A91 : male : divorced/separated\n",
        "#     - A92 : female : divorced/separated/married\n",
        "#     - A93 : male : single\n",
        "#     - A94 : male : married/widowed\n",
        "#     - A95 : female : single\n",
        "\n",
        "# - Attribute 10: (qualitative) **Other debtors / guarantors**\n",
        "#    - A101 : none\n",
        "#    - A102 : co-applicant\n",
        "#    - A103 : guarantor\n",
        "\n",
        "# - Attribute 11: (numerical) **Present residence since**\n",
        "\n",
        "# - Attribute 12: (qualitative) **Property**\n",
        "#     - A121 : real estate\n",
        "#     - A122 : if not A121 : building society savings agreement/ life insurance\n",
        "#     - A123 : if not A121/A122 : car or other, not in attribute 6\n",
        "#     - A124 : unknown / no property\n",
        "\n",
        "# - Attribute 13: (numerical) **Age in years**\n",
        "\n",
        "# - Attribute 14: (qualitative) **Other installment plans** Otros pagos por plazos\n",
        "#      - A141 : bank\n",
        "#      - A142 : stores\n",
        "#      - A143 : none\n",
        "\n",
        "# - Attribute 15: (qualitative) **Housing**\n",
        "#      - A151 : rent\n",
        "#      - A152 : own\n",
        "#      - A153 : for free\n",
        "\n",
        "# - Attribute 16: (numerical) **Number of existing credits at this bank**\n",
        "\n",
        "# - Attribute 17: (qualitative) **Job**\n",
        "#   - A171 : unemployed/ unskilled - non-resident\n",
        "#   - A172 : unskilled - resident\n",
        "#   - A173 : skilled employee / official\n",
        "#   - A174 : management/ self-employed/highly qualified employee/ officer\n",
        "\n",
        "# - Attribute 18: (numerical) **Number of people being liable to provide maintenance for**\n",
        "\n",
        "# - Attribute 19: (qualitative) **Telephone**\n",
        "#   - A191 : none\n",
        "#   - A192 : yes, registered under the customers name\n",
        "\n",
        "# - Attribute 20: (qualitative) **foreign worker**\n",
        "#   - A201 : yes\n",
        "#   - A202 : no\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4VlRVl60S1i"
      },
      "source": [
        "\n",
        "## Analisis univariante y de asociación con la variable objetivo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SeIS_8S0S1i"
      },
      "source": [
        "Que comprobar en el análisis descriptivo inicial?\n",
        "\n",
        "### **Análisis univariante**\n",
        "\n",
        "1- **Tipos de variables**. Todas las variables categóricas (factores) están bien identificadas?\n",
        "\n",
        "\n",
        "Para el análisis univariante es importante realizar un análisis gráfico con el **Histograma de Frecuencias**\n",
        "\n",
        "2- **Límites de las variables cuantitativas** Valores numéricos fuera de rango ¿Hay alguna limitación sobre el rango de alguna variable que no se cumpla?\n",
        "\n",
        "3- **Niveles de las variables cualitativas**, Valores mal codificados ¿los niveles de las variables cualitativas tienen sentido? ¿Hay missings no declarados tipo -1, 99999? Las categorías de las nominales son las que deben?\n",
        "\n",
        "4- **Variables nominales o categóricas o factores con categorías minoritarias**.Frecuencia de las categorías de las variables cualitativas.  Las categorías con baja representación puede causar muchos problemas en los modelos por falta de base muestral para la estimación de los parámetros correspondientes a la pertenencia a esa categoría. Por ello, es conveniente echar un vistazo y recodificar las vairables uniendo categorías muy poco representativas con otras cuya unión tenga algún sentido (tienen comportamiento similar frente a la objetivo, la variable tiene caracter ordinal por lo que la unión con mayor sentido sería hacia categorías adyacentes..).  Es imprescindible que todas los niveles de las variables cualitativas esten ´ bien representados pues, de lo contrario, se podr´ıan detectar patrones que no fueran extrapolables al estar basados en muy pocas observaciones.\n",
        " Por ello, se debe verificar que la frecuencia de todas ellas sea superior al 2-5 % (el porcentaje exacto depende del numero de observaciones del conjunto de datos) Yo prefiero que haya variabilidad suficiente: NO debe haber categorías con menos de un 5% de representación).\n",
        "\n",
        "5 **Variabilidad suficiente de las variables numéricas** Por encima de un 5% de valores distintos\n",
        "\n",
        "\n",
        "Con estas cosas ya arregladas, nos vamos a los dos grandes \"caballos de batalla\" de la depuración.\n",
        "\n",
        "5- **Outliers**. Incidencia y tratamiento (pasar a missing, eliminar, winsorizar o reemplazarlos con los valores no atípicos más cercanos)\n",
        "\n",
        "**Análisis de las variables/casos ausentes**\n",
        "\n",
        "\n",
        "6- **Missings**. Incidencia y tratamiento\n",
        "  - **Imputación** por valores validos (0-5%):  simple por media, mediana, aleatorio, imputación por modelos\n",
        "  - **Recategorizacion** (5%- 50%) de los valores missing como una categoría valida.\n",
        "  - **Eliminar** columnas u observaciones (superior al 50 %) Cuando en una variable hay mas de la mitad de los datos faltantes, es recomendable rechazarla al inicio del proceso, pues carece de suficiente informacion.\n",
        "  \n",
        "\n",
        "### **Análisis Bivariante**\n",
        "**Después del análisis Univariante se realiza el Análisis Bivariante, entre cada una de las potenciales variables explicativas y la Variable objetivo**\n",
        "\n",
        "a. Tablas de contigencia Chi2 (¿discretizar variables continuas V de Cramer o Chi2 dnormalizado entre 0 y 1,, independientes y totalmente dependientes respectivamente))\n",
        "b. Tablas de correlaciones\n",
        "c. Tablas Pivote (test de diferencia de medias)\n",
        "\n",
        "c) Métodos gráficos:\n",
        "  - Gráficos de dispersión\n",
        "  - Diagrama de Cajas o boxplot (o diagrama de barras)\n",
        "  - diagrams de mosaicos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th-S5dps0S1i"
      },
      "source": [
        "***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy20mLed0S1i"
      },
      "source": [
        "## Análisis exploratorio inicial de las **variables categóricas**\n",
        "\n",
        "Comenzamos con el análisis de las variables discretas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvb3mqpd0S1j",
        "outputId": "6cc22440-7833-40ba-c928-e29e0469d250"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "No objects to concatenate",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dt\u001b[38;5;241m.\u001b[39mdescribe(include\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:11995\u001b[0m, in \u001b[0;36mNDFrame.describe\u001b[1;34m(self, percentiles, include, exclude)\u001b[0m\n\u001b[0;32m  11753\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m  11754\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdescribe\u001b[39m(\n\u001b[0;32m  11755\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11758\u001b[0m     exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m  11759\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m  11760\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m  11761\u001b[0m \u001b[38;5;124;03m    Generate descriptive statistics.\u001b[39;00m\n\u001b[0;32m  11762\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11993\u001b[0m \u001b[38;5;124;03m    max            NaN      3.0\u001b[39;00m\n\u001b[0;32m  11994\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m> 11995\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m describe_ndframe(\n\u001b[0;32m  11996\u001b[0m         obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  11997\u001b[0m         include\u001b[38;5;241m=\u001b[39minclude,\n\u001b[0;32m  11998\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[0;32m  11999\u001b[0m         percentiles\u001b[38;5;241m=\u001b[39mpercentiles,\n\u001b[0;32m  12000\u001b[0m     )\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescribe\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\methods\\describe.py:97\u001b[0m, in \u001b[0;36mdescribe_ndframe\u001b[1;34m(obj, include, exclude, percentiles)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     describer \u001b[38;5;241m=\u001b[39m DataFrameDescriber(\n\u001b[0;32m     92\u001b[0m         obj\u001b[38;5;241m=\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m, obj),\n\u001b[0;32m     93\u001b[0m         include\u001b[38;5;241m=\u001b[39minclude,\n\u001b[0;32m     94\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[0;32m     95\u001b[0m     )\n\u001b[1;32m---> 97\u001b[0m result \u001b[38;5;241m=\u001b[39m describer\u001b[38;5;241m.\u001b[39mdescribe(percentiles\u001b[38;5;241m=\u001b[39mpercentiles)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(NDFrameT, result)\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\methods\\describe.py:175\u001b[0m, in \u001b[0;36mDataFrameDescriber.describe\u001b[1;34m(self, percentiles)\u001b[0m\n\u001b[0;32m    172\u001b[0m     ldesc\u001b[38;5;241m.\u001b[39mappend(describe_func(series, percentiles))\n\u001b[0;32m    174\u001b[0m col_names \u001b[38;5;241m=\u001b[39m reorder_columns(ldesc)\n\u001b[1;32m--> 175\u001b[0m d \u001b[38;5;241m=\u001b[39m concat(\n\u001b[0;32m    176\u001b[0m     [x\u001b[38;5;241m.\u001b[39mreindex(col_names, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ldesc],\n\u001b[0;32m    177\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    178\u001b[0m     sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    179\u001b[0m )\n\u001b[0;32m    180\u001b[0m d\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m d\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    383\u001b[0m     objs,\n\u001b[0;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m    385\u001b[0m     ignore_index\u001b[38;5;241m=\u001b[39mignore_index,\n\u001b[0;32m    386\u001b[0m     join\u001b[38;5;241m=\u001b[39mjoin,\n\u001b[0;32m    387\u001b[0m     keys\u001b[38;5;241m=\u001b[39mkeys,\n\u001b[0;32m    388\u001b[0m     levels\u001b[38;5;241m=\u001b[39mlevels,\n\u001b[0;32m    389\u001b[0m     names\u001b[38;5;241m=\u001b[39mnames,\n\u001b[0;32m    390\u001b[0m     verify_integrity\u001b[38;5;241m=\u001b[39mverify_integrity,\n\u001b[0;32m    391\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    393\u001b[0m )\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[1;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clean_keys_and_objs(objs, keys)\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[0;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[1;34m(self, objs, keys)\u001b[0m\n\u001b[0;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
            "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
          ]
        }
      ],
      "source": [
        "dt.describe(include='object')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giAwhdwV0S1j"
      },
      "source": [
        "### **status.of.existing.checking.account**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jj1UbdeW0S1j"
      },
      "outputs": [],
      "source": [
        "# tabla de distribución de frecuencia univariante\n",
        "dt['status.of.existing.checking.account'].value_counts(normalize=True,dropna=False).sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mPLVXal0S1j"
      },
      "outputs": [],
      "source": [
        "# tabla de contingencia: distribución de frecuencias bi-variante\n",
        "ctabla=pd.crosstab(dt['status.of.existing.checking.account'],dt['y'],margins=True).round(3)\n",
        "ctabla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "az_2f9yF0S1j"
      },
      "outputs": [],
      "source": [
        "# test Chi-cuadrado de independencia (Ho: ausencia de asociación)\n",
        "c, p, dof, expected = chi2_contingency(ctabla)\n",
        "# Print the p-value\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXlIfE-80S1k"
      },
      "outputs": [],
      "source": [
        "# tabla de contingencia agrupada por columnas (axis=1), Se puede analizar la importancia relativa comparando con los porcentajes clobales.\n",
        "pd.crosstab(dt['status.of.existing.checking.account'],dt['y'],margins=True, normalize=1).round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0g2B-IO0S1k"
      },
      "outputs": [],
      "source": [
        "# Porcentajes respecto al total de cada fila (normalize=0), habría que comparar con el porcentaje de eventos en el total de la muestra (30%)\n",
        "pd.crosstab(dt['status.of.existing.checking.account'],dt['y'],margins=True, normalize=0).round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3bN5IdI0S1k"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(dt['status.of.existing.checking.account'],dt['y'],margins=True, normalize=0).round(3).plot(figsize=(15,5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mm45d5lQ0S1k"
      },
      "outputs": [],
      "source": [
        "# Para comparar respecto a los totales\n",
        "\n",
        "pd.crosstab(dt['status.of.existing.checking.account'], dt['y'], margins=True, normalize=0).round(3).plot(figsize=(15, 5))\n",
        "plt.axhline(y=yT_0, color='#1f77b4', linestyle='--')\n",
        "plt.axhline(y=yT_1, color='#ff7f0e', linestyle='--')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezmj1ugD0S1k"
      },
      "source": [
        "### **Personal status and sex**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBGsXL4x0S1k"
      },
      "outputs": [],
      "source": [
        "dt['personal.status.and.sex'].value_counts(normalize=True,dropna=False).sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzAd5xzn0S1k"
      },
      "outputs": [],
      "source": [
        "ctabla=pd.crosstab(dt['personal.status.and.sex'],dt['y'],margins=True).round(3)\n",
        "ctabla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAY-BWT20S1l"
      },
      "outputs": [],
      "source": [
        "# test Chi-cuadrado de independencia (Ho: ausencia de asociación)\n",
        "c, p, dof, expected = chi2_contingency(ctabla)\n",
        "# Print the p-value\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcS1fijE0S1l"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(dt['personal.status.and.sex'],dt['y'],margins=True, normalize=0).round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cvm4zACT0S1l"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(dt['personal.status.and.sex'],dt['y'],margins=True, normalize=0).round(3).plot(figsize=(15,5))\n",
        "plt.axhline(y=yT_0, color='#1f77b4', linestyle='--')\n",
        "plt.axhline(y=yT_1, color='#ff7f0e', linestyle='--')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyJFR3Gq0S1v"
      },
      "source": [
        "### **Housing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfHlTNVf0S1w"
      },
      "outputs": [],
      "source": [
        "dt['housing'].value_counts(normalize=True,dropna=False).sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZP4S-PGk0S1w"
      },
      "outputs": [],
      "source": [
        "ctabla=pd.crosstab(dt['housing'],dt['y'],margins=True).round(3)\n",
        "ctabla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uegzXDG0S1w"
      },
      "outputs": [],
      "source": [
        "# test Chi-cuadrado de independencia (Ho: ausencia de asociación)\n",
        "c, p, dof, expected = chi2_contingency(ctabla)\n",
        "# Print the p-value\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFxMwOLc0S1w"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(dt['housing'],dt['y'],margins=True, normalize=0).round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXW6eXrZ0S1w"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(dt['housing'],dt['y'],margins=True, normalize=0).round(3).plot(figsize=(15,5))\n",
        "plt.axhline(y=yT_0, color='#1f77b4', linestyle='--')\n",
        "plt.axhline(y=yT_1, color='#ff7f0e', linestyle='--')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3p27Vnkd0S1w"
      },
      "source": [
        "### **Job**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sdMFsiQ0S1w"
      },
      "outputs": [],
      "source": [
        "dt['job'].value_counts(normalize=True,dropna=False).sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KM6iGQsM0S1w"
      },
      "outputs": [],
      "source": [
        "ctabla=pd.crosstab(dt['job'],dt['y'],margins=True).round(3)\n",
        "ctabla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-3xRPls0S1w"
      },
      "outputs": [],
      "source": [
        "# test Chi-cuadrado de independencia (Ho: ausencia de asociación)\n",
        "c, p, dof, expected = chi2_contingency(ctabla)\n",
        "# Print the p-value\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcjyuxZl0S1w"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(dt['job'],dt['y'],margins=True, normalize=0).round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vo6LyGGp0S1w"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(dt['job'],dt['y'],margins=True, normalize=0).round(3).plot(figsize=(15,5))\n",
        "plt.axhline(y=yT_0, color='#1f77b4', linestyle='--')\n",
        "plt.axhline(y=yT_1, color='#ff7f0e', linestyle='--')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lQQaYIP0S1w"
      },
      "source": [
        " ....  Habría que seguir haciendo esto con todas las variables categóricas para analizar asociación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CY95oBr0S1w"
      },
      "source": [
        "## Analisis univariante y de asociación con la variable objetivo de las **Variables continuas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XZ9e5sd0S1w"
      },
      "outputs": [],
      "source": [
        "dt.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRv6Fng90S1x"
      },
      "source": [
        "### **Credit.amount**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSNyPCwp0S1x"
      },
      "outputs": [],
      "source": [
        "sns.displot(x=(dt['credit.amount']), kind=\"kde\", fill=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jgLgRYD0S1x"
      },
      "source": [
        "Parece que como la mayoría de variables económicas tipo cantidad de dinero (precios, salarios, rentas, etc) se comportan como una log normal. Lo comprobamos gráficamente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGjJYKY70S1x"
      },
      "outputs": [],
      "source": [
        "sns.displot(x=np.log(dt['credit.amount']), kind=\"kde\", fill=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnkJlfNl0S1x"
      },
      "source": [
        "Vamos a transformar la variable 'credit.amount' en logaritmos para conseguir normalidad (o al menos que se parezca a una normal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obaOZ-uI0S1x"
      },
      "outputs": [],
      "source": [
        "dt['credit.amount']=np.log(dt['credit.amount'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Zz0cA5D0S1x"
      },
      "source": [
        "Ahora vamos a ver si hay diferencias en la distribución de la variable 'credit.amount' entre los buenos y los malos clientes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRQY_zYD0S1x"
      },
      "outputs": [],
      "source": [
        "sns.displot(x=(dt['credit.amount']), kind=\"kde\", fill=True, hue=dt.y, common_norm=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RK32lKl0S1x"
      },
      "source": [
        "Según el histograma sí parece haber asociación, al menos hay tres tramos bien diferenciados, uno primero donde es difícil diferenciar entre los buenos y malos, otro tramo donde hay una mayor cantidad relativa de buenos clientes, y un tercero, en el que a partir de una determinada cantidad de crédito hay una mayoría relativa de malos clientes.    \n",
        "\n",
        "Parece por tanto que sí hay asociación entre creditability (impago) y la cantidad de dinero que se solicita en el préstamo. PAra comprobarlo hacemos el test de diferencias de medias (Ho:igualdad de medias, esto es, ausencia de asociación)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UO7jhRR20S1x"
      },
      "outputs": [],
      "source": [
        "fvalue, pvalue = f_oneway(dt.loc[dt[\"y\"]==0,['credit.amount']], dt.loc[dt[\"y\"]==1,['credit.amount']])\n",
        "print(fvalue, pvalue)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4xA6iie0S1x"
      },
      "source": [
        "Hacemos la misma comprobación con el resto de variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5MYRZ5j0S1x"
      },
      "source": [
        "### **duration.in.month**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43-mcowQ0S1x"
      },
      "outputs": [],
      "source": [
        "sns.displot(x=(dt['duration.in.month']), kind=\"kde\", fill=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7f5j9La0S1x"
      },
      "outputs": [],
      "source": [
        "sns.displot(x=(dt['duration.in.month']), kind=\"kde\", fill=True, hue=dt.y, common_norm=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOs3tTCs0S1y"
      },
      "outputs": [],
      "source": [
        "fvalue, pvalue = f_oneway(dt.loc[dt[\"y\"]==0,['duration.in.month']], dt.loc[dt[\"y\"]==1,['duration.in.month']])\n",
        "print(fvalue, pvalue)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQTxYRXl0S1y"
      },
      "source": [
        "Tendría que continuar con el análisis del resto de variables continuas ....."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRmhOJN00S1y"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(dt, hue=\"y\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0UyhFZF0S1y"
      },
      "source": [
        "# **Selección de Variables**: análisis de Concentración para seleccionar las variables más **importantes** para meter en el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o78HQgPT0S1y"
      },
      "source": [
        "### Dividimos la muestra en entrenamiento y test     \n",
        "\n",
        "Comenzamos ya el proceso de construcción del modelo en sentido estricto. Por eso lo primero es partir la muestra para comprobar la bondad del modelo que estimemos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab7lg5H70S1y"
      },
      "outputs": [],
      "source": [
        "dt_train, dt_test = train_test_split(dt,stratify=dt[\"y\"], test_size=.25, random_state=1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVIGfC_R0S1y"
      },
      "outputs": [],
      "source": [
        "# Realizamos la trimificación optima de age.in.years\n",
        "variable=\"age.in.years\"\n",
        "X=dt_train[variable].values\n",
        "Y=dt_train['y'].values\n",
        "optb = OptimalBinning(name=variable, dtype=\"numerical\", solver=\"cp\")\n",
        "optb.fit(X, Y)\n",
        "optb.splits\n",
        "binning_table = optb.binning_table\n",
        "binning_table.build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qul-zDJ0S1y"
      },
      "outputs": [],
      "source": [
        "dt_train[\"y\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Far4tWOl0S1y"
      },
      "outputs": [],
      "source": [
        "dt_train[\"y\"].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8-3gw_70S1y"
      },
      "outputs": [],
      "source": [
        "dt_test[\"y\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUyHjInm0S1y"
      },
      "outputs": [],
      "source": [
        "dt_test[\"y\"].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl3Iz8-P0S1y"
      },
      "source": [
        "## Defino la tramificación óptima"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asDYaPwT0S1y"
      },
      "source": [
        "## Tramificación de la Variable: \"credit.amount\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2cRhmdT0S1y"
      },
      "outputs": [],
      "source": [
        "variable=\"credit.amount\"\n",
        "X=dt_train[variable].values\n",
        "Y=dt_train['y'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtVaL0bK0S1y"
      },
      "outputs": [],
      "source": [
        "optb = OptimalBinning(name=variable, dtype=\"numerical\", solver=\"cp\")\n",
        "\n",
        "# Si se quisiese fijar los intervalos manualmente (porque no gusten los que obtine el agoritmo, entonces habría que usar:\n",
        "#                     user_splits=\n",
        "#                     user_splits_fixed=\n",
        "# HAy veces que los datos tienen dátos missing y códigos especiales en este caso para obtener una categoría con esos datos missing y datos especiales hay que establecerlos\n",
        "#                     special_codes = [-9, -8, -7]\n",
        "\n",
        "# Una vez definido podemos pasar a estimarlo\n",
        "optb.fit(X, Y)\n",
        "optb.splits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INblyEMq0S1y"
      },
      "source": [
        "Nota: Por defecto se utiliza un arbol de clasificación para hacer una tramificación inicial, y después se aplica un proceso de optimización de agrupación de categorías para maximizar el Valor de Información"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y3fPoxn0S1z"
      },
      "source": [
        "Una vez realizado el proceso de tramificación y agrupación óptima de categorías, obtenemos la tabla de agrupación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzkJPCoZ0S1z"
      },
      "outputs": [],
      "source": [
        "binning_table = optb.binning_table\n",
        "binning_table.build()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEZbgMED0S1z"
      },
      "source": [
        "Cabe mencionar que el WOE en esta tabla parece estar definido al revés que lo hemos hecho en clase, por lo que el signo es justo el contrario al que cabría esperar según lo que hemos visto en clase. En particular por defecto `optbinning` define el WOE de una categoría $i$ como\n",
        "$$ WOE_i =  ln \\left ( {Non-event_i \\over Non-event_{total}} \\over {Event_i \\over Event_{total}}    \\right ) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us1eIjvD0S1z"
      },
      "source": [
        "En este sentido los niveles con mayor tasa de impagados tendrán un WOE menos, y a medida que se reduzca la tasa de impagados (mejor calidad crediticia) irá aumentando el WOE. De hecho, 'optbinning' ni siquiera utiliza la misma fórmula que yo he utilizado en clase, por lo que no está acotada entre cero y uno, puede valos más que uno sin que eso signifique sobre ajuste.\n",
        "\n",
        "Por talmotivo utilizaremos como criterio de selección exclusivamente IV<0.002"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asC8B31b0S1z"
      },
      "source": [
        "Podemos extraer el IV y el índice de Gini a partir de la tabla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-7DEjxt0S1z"
      },
      "outputs": [],
      "source": [
        "print(\"IV= \", binning_table.iv.round(3))\n",
        "print(\"Gini= \", binning_table.gini.round(3))\n",
        "\n",
        "# La última columna muestra el estadístico Jensen-Shannon de divergencia.\n",
        "# Es una medida de la similaridad entre dos distribuciones de probabilidad (frecuencias de buenos y malos )\n",
        "# que está acotada entre 0 y log2 (aprox 0.70) (puede utilizarse 0.01 como mínimo)\n",
        "print(\"JS= \", binning_table.js.round(3))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RwxYJ8e0S1z"
      },
      "outputs": [],
      "source": [
        "# Podemos profundizar en el análisis estimando otras\n",
        "binning_table.analysis(pvalue_test=\"chi2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9-aU4zP0S1z"
      },
      "outputs": [],
      "source": [
        "# Por ejemplo otra medida que suele utilizarse en el Quality score(QS) que está acotada entre 0 y 1 (puede utilizarse 0.01 como mínimo)\n",
        "print(\"QS= \", binning_table.quality_score.round(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Raz5VcTe0S1z"
      },
      "outputs": [],
      "source": [
        "# La tabla anterior también muestra la V de Cramer (por encima de 0.20 podría ser suficiente para decir que hay asociación, entre los tramos y el porcentaje de eventos\n",
        "\n",
        "# Pero también podemos realizar el test con la tabla de contigencia completa:Jensen-Sha\n",
        "x_transform_indices = optb.transform(X, metric=\"indices\")\n",
        "\n",
        "#pd.Series(x_transform_indices).value_counts(normalize=True,dropna=False).sort_index()\n",
        "ctabla=pd.crosstab(pd.Series(x_transform_indices),Y,margins=True).round(3)\n",
        "print(ctabla)\n",
        "\n",
        "# Chi-square test of independence. Ho: Ausencia de Asociación (independencia)\n",
        "c, p, dof, expected = chi2_contingency(ctabla)\n",
        "# Print the p-value\n",
        "print(\"Test independencia. Estadístico :\" ,round(c,3), \"p-valor:\", round(p,3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa8ZS3DF0S1z"
      },
      "source": [
        "Podemos realizar una representación gráfica de la Tabla de tramificación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_qaW4BM0S1z"
      },
      "outputs": [],
      "source": [
        "binning_table.plot(metric=\"woe\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ve9sbAsQ0S10"
      },
      "outputs": [],
      "source": [
        "binning_table.plot(metric=\"event_rate\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV6m89Ra0S10"
      },
      "source": [
        "Nótese que la relación entre lavariable x (credit.amount) y la tasa de evento (impago) es **no-lineal**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxmi_6qI0S10"
      },
      "source": [
        "Ahora podemos aplicar esta tramificación óptima a la variable original y obtener la variable transformada WOE (que será una variable continua que utilizaremos en el modelode regresión)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzm-SgRN0S10"
      },
      "outputs": [],
      "source": [
        "# Transformación WOE\n",
        "x_woe = optb.transform(X, metric=\"woe\")\n",
        "pd.Series(x_woe).value_counts().sort_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-KMwbFI0S10"
      },
      "source": [
        "Fíjate que ahora hemos conseguido \"linealizar\" la relación entre la variable trasnformada Woe y la propensión al impago"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pz1cOaf-0S10"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(x_woe,Y,normalize=0).round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpDq9Jig0S10"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(pd.crosstab(x_woe,Y,normalize=0).iloc[:,1])\n",
        "ax.set_xlabel(\"x_woe\")\n",
        "ax.set_ylabel(\"porcentaje de impago\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG0vChg80S10"
      },
      "source": [
        "Nótese que para hacer la validación deberíamos hacer exactamente la misma transformación WOE, con la misma tramificación, al conjunto test. Para ello debemos aplicar la transformación optima calculada con el conjunto de entrenamiento, pero sobre la muestra de validación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qz5ogDYU0S10"
      },
      "outputs": [],
      "source": [
        "# Transformación WOE en el conjunto test\n",
        "x_test_woe = optb.transform(dt_test[variable].values, metric=\"woe\")\n",
        "pd.Series(x_test_woe).value_counts().sort_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw0gb2DQ0S10"
      },
      "source": [
        "Nótese que **no** estamos calculando una nueva tramificación para el conjunto de test, sino aplicando la tramificación obtenida con el conjunto de entrenamiento.    \n",
        "En realidad si hiciéramos una tramificación óptima con el conjunto de test no tendría porqué salir igual que la estimada para el conunto de entrenamiento, como se puede comprobar a continuación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_jmbi1p0S10"
      },
      "outputs": [],
      "source": [
        "variable=\"credit.amount\"\n",
        "X_test=dt_test[variable].values\n",
        "Y_test=dt_test['y'].values\n",
        "optb_test = OptimalBinning(name=variable, dtype=\"numerical\")\n",
        "optb_test.fit(X_test, Y_test)\n",
        "print(optb_test.splits)\n",
        "binning_table_test = optb_test.binning_table\n",
        "binning_table_test.build()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaeemevD0S10"
      },
      "source": [
        "Nótese que con el conjunto de test se han obtenido sólo 6 tramos y con diferentes puntos de corte ( y diferentes WOE), por eso es necesario no hacer una nueva tramificación al conjnto de test sino aplicar la tramificación obtenida usando en el conjunto de entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AdywPnT0S10"
      },
      "source": [
        "## Tramificación de la duración en meses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Na4va9qU0S10"
      },
      "outputs": [],
      "source": [
        "variable=\"duration.in.month\"\n",
        "X=dt_train[variable].values\n",
        "Y=dt_train['y'].values\n",
        "\n",
        "optb = OptimalBinning(name=variable, dtype=\"numerical\", solver=\"cp\")\n",
        "\n",
        "optb.fit(X, Y)\n",
        "optb.splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gPevW5K0S10"
      },
      "outputs": [],
      "source": [
        "binning_table = optb.binning_table\n",
        "binning_table.build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNL10mj-0S10"
      },
      "outputs": [],
      "source": [
        "print(\"IV= \", binning_table.iv.round(3))\n",
        "print(\"Gini= \", binning_table.gini.round(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osWzYPGu0S10"
      },
      "outputs": [],
      "source": [
        "binning_table.plot(metric=\"woe\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MJall3X0S11"
      },
      "outputs": [],
      "source": [
        "binning_table.plot(metric=\"event_rate\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72Fur_6N0S11"
      },
      "outputs": [],
      "source": [
        "# Transformación WOE\n",
        "x_woe = optb.transform(X, metric=\"woe\")\n",
        "pd.Series(x_woe).value_counts().sort_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_zeDKD00S11"
      },
      "source": [
        "Habría que seguir con el resto de variables continuas....."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMH87Usl0S11"
      },
      "source": [
        "# Agrupción de niveles en variables Vbles Categóricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTcJV6Ax0S11"
      },
      "source": [
        "En realidad, cuando tenemosvariables categóricas, no es necesario tramificar, pero sí hacer una agrupación de los diferentes niveles de forma que se maximice el *valor de información*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs__9J3n0S11"
      },
      "source": [
        "## Agrupación de la variable  *purpose*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cduZAwD0S11"
      },
      "outputs": [],
      "source": [
        "variable_cat = \"purpose\"\n",
        "X_cat = dt_train[variable_cat].values\n",
        "Y_cat = dt_train['y'].values\n",
        "\n",
        "dt_train[variable_cat].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvG_2xeX0S11"
      },
      "outputs": [],
      "source": [
        "\n",
        "optb = OptimalBinning(name=variable_cat, dtype=\"categorical\", solver=\"cp\",\n",
        "                      cat_cutoff=0.1)  # podemos cambiar los valores por defecto cat_cutoff=None, o, cat_cutoff=0.005\n",
        "\n",
        "optb.fit(X_cat, Y_cat)\n",
        "optb.splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3S8QMa-O0S11"
      },
      "outputs": [],
      "source": [
        "binning_table = optb.binning_table\n",
        "binning_table.build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZuHvoNf0S11"
      },
      "outputs": [],
      "source": [
        "binning_table.plot(metric=\"event_rate\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEISQmLG0S11"
      },
      "outputs": [],
      "source": [
        "x_woe = optb.transform(X_cat, metric=\"woe\")\n",
        "pd.Series(x_woe).value_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmOKzP5Q0S11"
      },
      "source": [
        "... habría que seguir con el resto de variables categóricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifbJVVWt0S12"
      },
      "source": [
        "__________________________________________________________________________________________________________________________________________________________________________________________________\n",
        "__________________________________________________________________________________________________________________________________________________________________________________________________\n",
        "\n",
        "__________________________________________________________________________________________________________________________________________________________________________________________________\n",
        "__________________________________________________________________________________________________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VkGovBb0S12"
      },
      "source": [
        "# Proceso de tramificación, agrupación y trasformación WOE Completo\n",
        "\n",
        "Para no ir variable a variable se puede hacer todo el proceso completo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80xPWzqc0S12"
      },
      "source": [
        "Proceso Entero\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXUYQ0KP0S12"
      },
      "outputs": [],
      "source": [
        "# 1) Definimos la lista de nombres señalando cualse de ellas son las categóricas\n",
        "Y = dt_train['y'].values\n",
        "X = dt_train.drop(columns=['y']) #todas menos la primera que es el ID y la variable y\n",
        "list_variables = X.columns.values.tolist()\n",
        "list_categorical = X.select_dtypes(include=['object', 'category']).columns.values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6wfZ7Ke0S12"
      },
      "outputs": [],
      "source": [
        "# 2) Definimos el criterio de selección\n",
        "selection_criteria = {\n",
        "    \"iv\": {\"min\": 0.02}  # no imponemos \"max\": 1}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54kZUZZU0S12"
      },
      "outputs": [],
      "source": [
        "# En caso de que desee modificarse los valores por defecto en el proceso de tramificación de alguna variable puede hacerse en forma de diccionario\n",
        "\n",
        "binning_fit_params={\n",
        "    \"purpose\":{\"cat_cutoff\": 0.10}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q17Oz49c0S12"
      },
      "outputs": [],
      "source": [
        "# 3) Definimos el proceso de Tramificación o BinningProcess\n",
        "binning_process = BinningProcess(\n",
        "    categorical_variables=list_categorical,\n",
        "    variable_names=list_variables,\n",
        "    selection_criteria=selection_criteria,\n",
        "    binning_fit_params=binning_fit_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e24HqWsY0S12"
      },
      "outputs": [],
      "source": [
        "# 4) Obtenemos los tramos optimos de todas las Variables\n",
        "dt_train_binned = binning_process.fit(X, Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2K1r2I8O0S12"
      },
      "outputs": [],
      "source": [
        "dt_train_binned.summary().sort_values('iv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhi1UwCB0S12"
      },
      "outputs": [],
      "source": [
        "# Ahora podemos ir sacando las tablas para cada variable\n",
        "\n",
        "dt_train_binned.get_binned_variable(\"credit.amount\").binning_table.build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxs8vNc40S12"
      },
      "outputs": [],
      "source": [
        "dt_train_binned.get_binned_variable(\"purpose\").binning_table.build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuqPaFI50S12"
      },
      "outputs": [],
      "source": [
        "dt_train_binned.information()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lZ3MoQY0S12"
      },
      "outputs": [],
      "source": [
        "# las variables seleccionadas se pueden obtener con 'get_support'\"Tarea Estudiantes_TarjetaPuntuacion\"\n",
        "dt_train_binned.get_support()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQd41MV70S12"
      },
      "outputs": [],
      "source": [
        "# Podemos transformar las variables WOE\n",
        "dt_train_woe=dt_train_binned.transform(X, metric=\"woe\")\n",
        "\n",
        "\n",
        "# Existe la posibilidad de obtener directamente las transformada si en lugar de usar fit, hubiésemos usado fit_transform\n",
        "# dt_train_binned = binning_process.fit_transform(X, Y)\n",
        "# dt_train_binned.info()\n",
        "# el resultado sería un data.frame con las X seleccionadas trasnsformadas WOE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRHkN5Qk0S12"
      },
      "outputs": [],
      "source": [
        "dt_train_woe.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFANbceD0S12"
      },
      "outputs": [],
      "source": [
        "dt_train_woe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cE8oW2p50S13"
      },
      "outputs": [],
      "source": [
        "# Ahora aplicaríamos la misma transformación pero al conjunto de test (si hubiera que puntuar a nuevos clientes haríamos lo mismo)\n",
        "\n",
        "Y_test = dt_test['y'].values\n",
        "X_test = dt_test.drop(columns=['y']) #todas menos la primera que es el ID y la variable y\n",
        "\n",
        "dt_test_woe=dt_train_binned.transform(X_test, metric=\"woe\")\n",
        "dt_test_woe.info()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6i97RNMg0S13"
      },
      "outputs": [],
      "source": [
        "dt_test_woe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdE0KURy0S13"
      },
      "source": [
        "# Estimación del Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkJwZJfV0S13"
      },
      "source": [
        "Ahora podemos calcular la tarjeta de puntuación. En los apuntes de clase definimos tanto los WOE, como los Odd ratio como la probabilidad de `evento` respecto al `no-evento` (malos clientes o impago=1 respecto a los buenos clientes o impago=0):\n",
        "\n",
        "$$ odd = {{P}\\over {(1-P)}} ~~ {,~~  siendo} ~~  {P=Prob(impago=1)} $$\n",
        "\n",
        " Y la fórmula para obtener la puntuación o los score debe ser una relación negativa con los odd ratio: cuanto mayor la probabilidad de impago (en relación a la de no impago), menor puntuación ha de tener:\n",
        "\n",
        " $$ score {= offset - Factor}~·~{ln(odds)}$$\n",
        "\n",
        "Para pasar de Probabiliddes de impago a Puntuaciones, habrá que establecer tanto el valor de `offset` como el de `Factor`. Esto se hace de manera arbitraria dependiendo de cada institución financiera.\n",
        "\n",
        "En general, para determinar estos dos valores es necesario establecer la pendiente de la recta y un punto de la misma.\n",
        "\n",
        "En cuanto a la pendiente, cuanto más plana sea la pendiente, menor variabilidad tendrán los valores de puntuación de crédito que se alcancen, y al revés, cuanto mayor pendiente más diferencias en la puntuación final. Yo voy a utilizar un apendiente (arbitraria) estableciendo de forma arbitraria cada cuantos puntos de score (**pdo_0**) se dobla el odd ratio: $ score - pdo_0 = {offset -Factor}~ ·{ln(2*odds)}$.\n",
        "\n",
        " En cuanto al punto de la recta (arbitrario), puede hacerse estableciento (de manera arbitraria) la puntuación o score considerada como de sobresaliente(**scorecard_points**) y el odd ratio que debería tener ese cliente de *sobresaliente* (**odds_0**)\n",
        "\n",
        " Así habría que establecer tres parámetros para transformar probabilidades de impago a puntuaciones, por ejemplo:   \n",
        "\n",
        "* **pdo_0** =40  (esto es que cada 40 puntos de calidad creditica se dobla el odd-ratio))\n",
        "* **scorecard_points** =600  (alguien con calidad crediticia muy buena, de sobresaliente, sacaría 600 puntos)\n",
        "* **odds_0** =1/50  (odd ratio que se considera de sobresaliente)\n",
        "\n",
        "La librería `optBinning` [librería OptBinning](http://gnpalencia.org/optbinning/), en realidad utiliza el módulo de `credit scoring` de `SAS-miner` como inspiración, y por eso define al revés tanto los WOE como los odd ratio, es decir `no-evento` en relación a `evento` (clientes buenos respecto a los malos, o no-impago respecto a impago, impago=0 respecto a impago=1).\n",
        "$$ odd^B = {{(1-P)}\\over {P}} ~~ {,~~  siendo} ~~  {P=Prob(impago=1)} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXx5uoK-0S13"
      },
      "source": [
        "Esto implica que la ecuación que transforma las probabilidades de impago en scores utilizando esta *odds<sup>B</sup>* debe tener pendiente positiva (cuanto mejor *odd<sup>B</sup>* mejor calidad crediticia tiene el cliente)\n",
        "\n",
        " $$ score= {offset + Factor} ~·~ {ln(odds^B)}$$\n",
        "\n",
        " Nótese que ahora habrá que establecer de nuevo los puntos de score que doblan el odd ratio (**pdo_0**), y también la puntuación o score considerada como de sobresaliente(**scorecard_points**) y el odd ratio que debería tener ese cliente de *sobresaliente* **odds_0 <sup>*B*</sup>**, con **odds_0 <sup>*B*</sup>** **= 1/odds_0**.\n",
        "\n",
        " Así para estimar la puntuación crediticia con `optBinning` hay que establecer tres parámetros para transformar probabilidades de impago a puntuaciones, por ejemplo:   \n",
        "\n",
        "* **pdo_0** =40\n",
        "* **scorecard_points** =600\n",
        "* **odds_0 <sup>*B*</sup>** = 50  (equivalente a **odds_0** =1/50 )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvZoK0Nn0S13"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Directamente con el método Scorecard\n",
        "estimator = LogisticRegression(solver=\"lbfgs\")\n",
        "\n",
        "# Establecemos los parámetros para la transformación de probabilidades en puntos de calidad crediticia o score\n",
        "\n",
        "pdo_0 =40\n",
        "scorecard_points_0= 600\n",
        "odds_0_B= 50 # (equivalente a  odds_0 =1/50 )\n",
        "\n",
        "tarjeta= Scorecard(binning_process=binning_process,\n",
        "                   estimator=estimator,\n",
        "                   scaling_method=\"pdo_odds\",\n",
        "                   scaling_method_params={\"pdo\":pdo_0, \"odds\": odds_0_B, \"scorecard_points\": scorecard_points_0})\n",
        "\n",
        "tarjeta.fit(X, Y, show_digits=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RzdcZZT0S13"
      },
      "outputs": [],
      "source": [
        "# Podemos obtener los parámetros del modelo de regresión logística\n",
        "tarjeta.table(style=\"detailed\")[[\"Variable\",\"Coefficient\"]].groupby([\"Variable\", \"Coefficient\"]).nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJYESXCO0S13"
      },
      "source": [
        "Y podemos obtener los puntos de la tarjeta de puntuación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFmtxk1A0S13"
      },
      "outputs": [],
      "source": [
        "tarjeta.table().head(60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zz9ziI40S13"
      },
      "outputs": [],
      "source": [
        "#Aquí se obtienen todos los estadísticos\n",
        "tarjeta.table(style=\"detailed\").head(16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0KclW7N0S13"
      },
      "source": [
        "Continuamos con la Diagnosis del Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrmeboRk0S13"
      },
      "outputs": [],
      "source": [
        "# obtenmos las predicciones\n",
        "Y_pred=tarjeta.predict_proba(X)[:,1]\n",
        "\n",
        "# Calculamos la media\n",
        "Y_pred.mean().round(4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B27IihD0S13"
      },
      "source": [
        "Para contruir las matrices de confusión necesitamos determinar un **punto de corte de la probabilidad**.    \n",
        "\n",
        "Ese punto de corte es el que me va a ayudar a realizar un pronóstico sobre los clientes: los malos clientes serán aquellos para los que Prob Estimada > Prob_corte.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aGNJ9XR0S13"
      },
      "outputs": [],
      "source": [
        "# Para elegir el punto de corte puede utilizarse el Plot Kolmogorov-Smirnov (KS)\n",
        "plot_ks(Y, Y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca2QnnmS0S13"
      },
      "outputs": [],
      "source": [
        "# También puede utilizarse el máximo del f1_score\n",
        "# definimos un vector de puntos de corte\n",
        "c=np.arange(0,1,0.01)\n",
        "# calculamos el f1_score para cada punto de corte\n",
        "f1_score_ = [f1_score(dt_train[\"y\"],np.multiply(Y_pred>c_,1)) for c_ in c]\n",
        "# obtenemos el punto de corte que maximiza el f1_score\n",
        "c_max = c[np.argmax(f1_score_)]\n",
        "print(\"El punto de corte que maximiza el f1_score es: \", c_max)\n",
        "print(\"y el máximo se alcanza en \", np.max(f1_score_).round(3))\n",
        "\n",
        "# hacemos un gráfico de c y los f1_score correspondientes\n",
        "plt.plot(c,f1_score_)\n",
        "plt.stem(c_max, np.max(f1_score_),linefmt='r--', markerfmt='ro', basefmt='r--')\n",
        "plt.title(\"Gráfico f1_score vs diferentes puntos de corte\")\n",
        "plt.text(c_max+0.05, 0, \"c_max = \"+str(c_max.round(2))+\"\\n f1_score_max = \"+str(np.max(f1_score_).round(3)))\n",
        "plt.show()\n",
        "\n",
        "c_maxF1=c_max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcFZEVdb0S13"
      },
      "outputs": [],
      "source": [
        "# método de Youden (J) para obtener el punto de corte óptimo\n",
        "# definimos un vector de puntos de corte (c) y de probabilidad de aceptación (p)\n",
        "c=np.arange(0,1,0.01)\n",
        "\n",
        "# Calculamos el estadístico J de Youden para cada punto de corte= Sensibilidad + Especificidad -1\n",
        "J= [balanced_accuracy_score(dt_train[\"y\"],np.multiply(Y_pred>c_,1), adjusted=True) for c_ in c ]\n",
        "# obtenemos el punto de corte que maximiza el índice de Youden\n",
        "c_max = c[np.argmax(J)]\n",
        "print(\"El punto de corte que maximiza el índice de Youden es: \", c_max)\n",
        "print(\"y el máximo se alcanza en \", np.max(J).round(3))\n",
        "\n",
        "# gráfico de c y los índices de Youden correspondientes\n",
        "plt.plot(c,J)\n",
        "plt.stem(c_max, np.max(J),linefmt='r--', markerfmt='ro', basefmt='r--')\n",
        "plt.title(\"Gráfico índice de Youden vs diferentes puntos de corte\")\n",
        "plt.text(c_max+0.05, 0, \"c_max = \"+str(c_max.round(2))+\"\\n J_max = \"+str(np.max(J).round(3)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPUgCqSN0S14"
      },
      "outputs": [],
      "source": [
        "# Para el punto de corte se podría utilizar simplemente la frecuencia observada, o utilizar el punto donde se alcanza el maximo del F1 Score\n",
        "Prob_Corte=Y.mean()\n",
        "# Prob_Corte=c_maxF1\n",
        "print(' Punto de Corte seleccionado:', Prob_Corte.round(2),'\\n',\n",
        "      'Frecuencia media de eventos (y=1):', Y.mean().round(2), '\\n',\n",
        "      'máximo del F1-score :', c_maxF1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GnGO2r-0S14"
      },
      "source": [
        "Ahora ya podemos hacer los pronósticos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rYHWwZq0S14"
      },
      "outputs": [],
      "source": [
        "dt_train[\"Y_pronostico\"]=np.multiply(Y_pred>Prob_Corte,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHc9yAjw0S14"
      },
      "source": [
        "Para comprobar la bondad de nuestras predicciones voy a comparar resultados con la tabla de confusión"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WZ1z6vI0S14"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Primero estimo la precisión (los aciertos sobre el total de mis pronósticos)\n",
        "pd.crosstab(dt_train[\"y\"],dt_train[\"Y_pronostico\"],margins=True, normalize=1).round(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiMYxN-i0S14"
      },
      "source": [
        "Nótese que los falsos negativos (Bad Rate) es del 11.5% (préstamos aceptados o pronosticados como buenos que resultaron impagados)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0dvSolU0S14"
      },
      "outputs": [],
      "source": [
        "# Ahora estimo la exhaustividad o recall (Aciertos sobre los casos reales):\n",
        "# la sensibilidad (sobre los positivos y=1), y la Especificidad (sobre los negativos y=0)\n",
        "\n",
        "pd.crosstab(dt_train[\"y\"],dt_train[\"Y_pronostico\"],margins=True, normalize=0).round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOAaidZ70S14"
      },
      "outputs": [],
      "source": [
        "# Por último un resumen global de aciertos\n",
        "f1_score(dt_train[\"y\"],dt_train[\"Y_pronostico\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbZvK1ue0S14"
      },
      "source": [
        "Para hacer la diagnosis también puedo utilizar medidas que no dependan crucialmente de un único punto de corte de Probabilidad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vb_zMyc-0S14"
      },
      "outputs": [],
      "source": [
        "# Diagnosis Curva ROC\n",
        "plot_auc_roc(Y,Y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w93wC6cL0S14"
      },
      "outputs": [],
      "source": [
        "# Diagnosis Cumulative Accuracy Profile (CAP)\n",
        "# Otra curva alternativa a la curva ROC que permite evaluar la bondad de un modelo de clasificación es la curva CAP (Cumulative Accuracy Profile).\n",
        "# La curva CAP se construye de la siguiente manera: ordenamos las observaciones de mayor a menor probabilidad\n",
        "# de pertenecer a la clase positiva (y=1). A continuación, vamos acumulando las observaciones y calculando\n",
        "# la proporción de positivos acumulados sobre el total de positivos. Esta proporción se representa en el eje Y.\n",
        "\n",
        "# En el eje X representamos la proporción de observaciones acumuladas sobre el total de observaciones.\n",
        "# La curva CAP se construye a partir de la curva de la línea recta (curva de referencia) y\n",
        "# la curva de la línea que representa la probabilidad estimada por el modelo.\n",
        "\n",
        "plot_cap(Y, Y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJPHvc2J0S14"
      },
      "outputs": [],
      "source": [
        "#### OJO que la diagnosis debe hacerse fuera de la muestra de entrenamiento\n",
        "# obtenmos las predicciones\n",
        "Y_test_pred=tarjeta.predict_proba(X_test)[:,1]\n",
        "\n",
        "# Calculamos la media\n",
        "Y_test_pred.mean().round(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-_NUAic0S14"
      },
      "outputs": [],
      "source": [
        "# Diagnosis Curva ROC\n",
        "plot_auc_roc(Y_test,Y_test_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcYgOfS00S14"
      },
      "outputs": [],
      "source": [
        "plot_cap(Y_test,Y_test_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbwM_P2C0S14"
      },
      "outputs": [],
      "source": [
        "dt_test[\"Y_pronostico\"]=np.multiply(Y_test_pred>Prob_Corte,1)\n",
        "\n",
        "# Primero estimo la precisión (los aciertos sobre el total de mis pronósticos)\n",
        "print(\"\\n Precisión:\\n\", pd.crosstab(dt_test[\"y\"],dt_test[\"Y_pronostico\"],margins=True, normalize=1).round(3))\n",
        "\n",
        "# Ahora estimo la exhaustividad o recall (Aciertos sobre los casos reales):\n",
        "# la sensibilidad (sobre los positivos y=1), y la Especificidad (sobre los negativos y=0)\n",
        "print(\"\\n exhaustividad:\\n\",pd.crosstab(dt_test[\"y\"],dt_test[\"Y_pronostico\"],margins=True, normalize=0).round(3))\n",
        "\n",
        "# Por último un resumen global de aciertos\n",
        "print(\"\\n f1-score:\",f1_score(dt_test[\"y\"],dt_test[\"Y_pronostico\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtoN01ey0S14"
      },
      "source": [
        "Pasamos ahora de Probabilidades a Puntuaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pk-ufD30S15"
      },
      "outputs": [],
      "source": [
        "# Ahora vamos a calcular los score o puntuaciones.\n",
        "# Que podemos hacer  con la función score\n",
        "score = tarjeta.score(X)\n",
        "\n",
        "print(\"Puntuación mínima: \", score.min().round(2))\n",
        "print(\"Puntuación máxima: \",score.max().round(2))\n",
        "print(\"Puntuación media : \",score.mean().round(2))\n",
        "\n",
        "\n",
        "# O también haber calculado la puntuación manualmente (voy a calcularla para comprobar que da exactamente el mismo resultado)\n",
        "\n",
        "# Transformación lineal según apuntes\n",
        "# Factor= (pdo_0/log(2))\n",
        "# Offset = scorecard_points_0+(pdo_0/log(2))*log(odds0_0)\n",
        "# score= Offset - Factor *log(odds)\n",
        "\n",
        "Factor= (pdo_0/np.log(2))\n",
        "Offset = scorecard_points_0+Factor*np.log(1/odds_0_B)\n",
        "\n",
        "score2= Offset-Factor*np.log(Y_pred/(1-Y_pred))\n",
        "\n",
        "# Podemos comprobar que los resultados son los mismos\n",
        "print(\"Puntuación mínima cálculo manual: \",score2.min().round(2))\n",
        "print(\"Puntuación máxima cálculo manual: \",score2.max().round(2))\n",
        "print(\"Puntuación media cálculo manual : \",score2.mean().round(2))\n",
        "\n",
        "datos_score=pd.DataFrame(np.transpose([score,score2, Y,Y_pred]), columns=['score','scoreManual','Y','Y_pred'])\n",
        "\n",
        "# si es necesario podemos guardar los datos\n",
        "# datos_score.to_excel(\"score_p1.xlsx\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRr9Mx9R0S15"
      },
      "source": [
        "Saber cómo se hace la transformación manual puede ayudarnos por ejemplo a la hora de establecer la `nota que determina el aprobado`. Imaginemos que utilizamos la frecuencia observada de impagos como probabilidad de corte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkacPKTX0S15"
      },
      "outputs": [],
      "source": [
        "\n",
        "Score_Corte= Offset-Factor*np.log(Prob_Corte/(1-Prob_Corte))\n",
        "\n",
        "print(\"La probabilida de corte de: \", Prob_Corte, \" equivale a una puntuación de corte de: \", Score_Corte.round(2) )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvBsKsf40S15"
      },
      "outputs": [],
      "source": [
        "# Ahora representamos en un gráfico cómo separa el modelo a los buenos y los malos\n",
        "datos_score=pd.DataFrame(np.transpose([score,Y]), columns=['score','Y'])\n",
        "sns.displot(data=datos_score, x='score', label=\"event\", hue='Y', alpha=0.35,kind=\"kde\", fill=True, common_norm=True)\n",
        "plt.axvline(Score_Corte, color='k', linestyle=\":\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iSs9JdQ0S15"
      },
      "source": [
        "# Seguimiento del modelo: PSI (Population Stability Index)\n",
        "El PSI es una medida de diferencia en la distribución de dos muestras, en nuestro caso entre la muestra utilizada para construir el modelo (entrenar y validar el modelo), y los nuevos datos que se vayan obteniendo con el transcurso del tiempo.  \n",
        "\n",
        "Se aplica para detectar cuándo comienzan a verse diferencias entre las dos muestras (las puntuaciones de la muestra -train- y las puntuaciones obtenidas con los nuevos datos .... Cuando las distribuciones dejen de parecerse será el momento de revisar el modelo a tenor de los nuevos datos\n",
        "\n",
        "Como regla general\n",
        "  - **PSI <0.1**: No hay diferencias significativas entre las muestras de entrenamiento y los nuevos datos (resultado deseado, no se requiere más acciones)\n",
        "  - **PSI entre 0.1 y 0.25** Hay cambio menores, valdría la pena revisar el modelo\n",
        "  - **PSI >0.25** hay cambios importantes entre las dos muestras HAY QUE CAMBIAR EL MODELO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKQ8VPLQ0S15"
      },
      "outputs": [],
      "source": [
        "# Supongamos que tenemos un conjunto de nuevos datos que hemos ido recopilando después de la puesta en producción del modelo,\n",
        "# y queremos utilizar esos nuevos datos para saber si es necesario revisar el modelo o si por el contrario podemos seguir utilizándolo\n",
        "\n",
        "# Como en la base de datos no disponemos de este tipo de datos voy a suponer simplemente que los datos de test son los nuevos datos,\n",
        "\n",
        "dt_nuevosdatos= dt_test.copy()\n",
        "\n",
        "\n",
        "# Valores nuevos\n",
        "Y_nuevo = dt_nuevosdatos['y'].values\n",
        "X_nuevo = dt_nuevosdatos.drop(columns=['y']) #todas menos la primera que es el ID y la variable y\n",
        "\n",
        "# Valores de entrenamiento\n",
        "Y = dt_train['y'].values\n",
        "X = dt_train.drop(columns=['y']) #todas menos la variable y\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoKLOxEL0S15"
      },
      "outputs": [],
      "source": [
        "# ¿se distibuyen igual las probabilidades esperadas?\n",
        "score_train = tarjeta.score(X)\n",
        "score_nuevo = tarjeta.score(X_nuevo)\n",
        "\n",
        "datos_score_psi1=pd.DataFrame(np.transpose([score_train,Y]), columns=['score','Y'])\n",
        "datos_score_psi1['tipo']='entrenamiento'\n",
        "\n",
        "datos_score_psi2=pd.DataFrame(np.transpose([score_nuevo,Y_nuevo]), columns=['score','Y'])\n",
        "datos_score_psi2['tipo']='nuevos datos'\n",
        "\n",
        "datos_score_psi= pd.concat([datos_score_psi1,datos_score_psi2])\n",
        "sns.displot(data=datos_score_psi, x='score', label=\"event\", hue='tipo', alpha=0.35,kind=\"kde\", fill=True,common_norm=False)\n",
        "plt.axvline(Score_Corte, color='k', linestyle=\":\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mxhqpaah0S15"
      },
      "source": [
        "Aparentemente las distribuciones son parecidas, eso quiere decir que los nuevos datos se parecen a los utilizados en el entrenamiento del modelo, por lo que seguramente no es necesario revisar el modelo. Lo comprobamos de todas formas con el estadístico PSI (Population Stability Index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wwgitlg0S15"
      },
      "outputs": [],
      "source": [
        "# Estimo el psi\n",
        "# Defino la tarjeta a evalear\n",
        "# psi=ScorecardMonitoring(tarjeta, psi_method=\"cart\",psi_min_bin_size=0.05, psi_n_bins=13)\n",
        "psi=ScorecardMonitoring(tarjeta, psi_method= \"quantile\", psi_n_bins=10)\n",
        "psi.fit(X_actual=X_nuevo, y_actual=Y_nuevo, X_expected=X, y_expected=Y)\n",
        "\n",
        "psi.psi_plot()\n",
        "psi.psi_table()\n",
        "psi.tests_table()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NF8v0w-O0S15"
      },
      "outputs": [],
      "source": [
        "psi.system_stability_report()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkvAxq5Y0S15"
      },
      "outputs": [],
      "source": [
        "psi.psi_variable_table(style=\"summary\").sort_values('PSI')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "5043996dbb5012f6a1a52b57fe18efa9888e0de81faf0d7ab7fe229615f6c3c4"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}